# 分词器

## 规范化normalization

提高文档的查询成功率，对词进行切分、同义词归并、大写转小些、负数转单数等；主要分为4步：

1.  切词(word segmentation)
2. 规范化：normalization
3. 去重
4. 字典序

## 字符过滤器character filter

1. #### HTML Strip：处理html标签

   ```json
   创建分词器
   # analysis表示分析器
   # char_filter表示字符串过滤器，过滤器类型是html_strip
   # my_analyzer表示分词
   # escaped_tags表示希望保留的标签
   PUT my_index
   {
     "settings": {
       "analysis": {
         "char_filter": {
           "my_char_filter":{
             "type":"html_strip"
           }
         },
         "analyzer": {
           "my_analyzer":{
             "tokenizer":"keyword",
             "char_filter": "my_char_filter"
           }
         }
       }
     }
   }
   
   # 使用分词器
   GET my_index/_analyze
   {
     "analyzer": "my_analyzer",
     "text":"<p>I'm so <a>happy</a></p>"
   }
   
   
   # 返回的结果
   {
     "tokens" : [
       {
         "token" : """
   
   I'm so <a>happy</a>
   
   """,
         "start_offset" : 0,
         "end_offset" : 26,
         "type" : "word",
         "position" : 0
       }
     ]
   }
   ```

2. #### Mapping Character Filter

   ```json
   # 使用分词器
   GET my_index/_analyze
   {
     "analyzer": "my_analyzer",
     "text":"<p>I'm so <a>happy</a></p>"
   }
   
   
   
   PUT my_index
   {
     "settings": {
       "analysis": {
         "char_filter": {
           "my_char_filter":{
             "type":"mapping",
             "mappings":[
               "滚 => *",
               "垃圾 => **"
               ]
           }
         },
         "analyzer": {
           "my_analyzer":{
             "tokenizer":"keyword",
             "char_filter": "my_char_filter"
           }
         }
       }
     }
   }
   # 使用分词器
   GET my_index/_analyze
   {
     "analyzer": "my_analyzer",
     "text":"垃圾应该滚去垃-级堆"
   }
   
   # 返回的结果
   {
     "tokens" : [
       {
         "token" : "**应该*去垃-级堆",
         "start_offset" : 0,
         "end_offset" : 10,
         "type" : "word",
         "position" : 0
       }
     ]
   }
   ```

3. #### Pattern Replace

   ```json
   # 正则替换
   PUT my_index
   {
     "settings": {
       "analysis": {
         "char_filter": {
           "my_char_filter":{
             "type":"pattern_replace",
             "pattern":"(\\d{3})\\d{4}(\\d{4})",
             "replacement":"$1****$2"
           }
         },
         "analyzer": {
           "my_analyzer":{
             "tokenizer":"keyword",
             "char_filter": "my_char_filter"
           }
         }
       }
     }
   }
   # 使用分词器
   GET my_index/_analyze
   {
     "analyzer": "my_analyzer",
     "text":"手机号是18764077883"
   }
   ```

## 分词器 tokenizer

## 令牌过滤器 token filter

## 常用分词器

## 中文分词器

ik分词器简单示例

```json
GET _analyze
{
  "text":"你好，吃早饭了吗",
  "analyzer": "ik_smart"
}
```

## 自定义分词器

```json

# 自定义分词器
PUT custom_analysis
{
  "settings": {
    "analysis": {
      "char_filter": {
        "my_char_filter": {
          "type": "mapping",
          "mappings": [
            "滚 => *",
            "垃圾 => **"
          ]
        }
      },
      "filter": {
        "my_stopword": {
          "type": "stop",
          "stopwords": [
            "is",
            "in",
            "the",
            "a",
            "at",
            "for"
          ]
        }
      },
      "tokenizer":{
        "my_tokenizer":{
          "type":"pattern",
          "pattern":"[,.!?]"
        }
      },
      "analyzer": {
        "my_analyzer":{
          "type":"custom",
          "char_filter":["my_char_filter"],
          "tokenizer":"my_tokenizer",
          "filter":["my_stopword"]
        }
      }
    }
  }
}

# 测试分词器
GET custom_analysis/_analyze
{
  "analyzer": "my_analyzer",
  "text":["what is asdf,.? ss in & | is the good boy滚hh"]
}

```



## 热更新